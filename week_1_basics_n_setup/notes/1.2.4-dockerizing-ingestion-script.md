# 1.2.4 - Dockerizing the Ingestion Script

Recall: Talked about Docker, and Postgres in Docker. Looked at taxi ride data and loaded database with it
- In previous video, we connected Postgres with pgAdmin with Docker network

Now we will turn the [upload-data.ipynb](../2_docker_sql/upload-data.ipynb) into a script [ingest_data.py](../2_docker_sql/ingest_data.py) and put data in PostgreSQL via Docker.

## Converting Notebook
Using `jupyter` in the CLI (activate your environment), convert notebook to script with:
- `jupyter nbconvert --to=script upload-data.ipynb`
- Has the same notebook content in the Python script.
    - Clean it by removing cell lines and clean up imports
    - Remove unecessary prints and markdown
    - Remove line magic

Add `argparse` library so we can have named arguments like `host`, `password`, etc
- Add these in an `if __name__ == '__main__':` clause
- Define parser: `parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')`
- Define arg: `parser.add_argument('--user', required=True, help='user name for postgres')`
    - Do this for user, password (can't use 'pass'), host, port, db, table name, and URL
- Parsed object:     `args = parser.parse_args()`

Load the variables from the parser in `main()` block
- Load to connection with `create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')`
- Download CSV with `os.system(f"wget {url} -O {csv_name}")`
    - Get the name with:
        ```
        # the backup files are gzipped, and it's important to keep the correct extension
        # for pandas to be able to open the file
        if url.endswith('.csv.gz'):
            csv_name = 'output.csv.gz'
        else:
            csv_name = 'output.csv'
        ```
    - Calls `wget` from host Linux terminal
- In pgAdmin run `DROP TABLE yellow_taxi_data;` first
    - Confirm by running any command on the table

Bash command would be:
```
URL="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
python ingest_data.py \
    --user=root \
    --password=root \
    --host=localhost \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_data \
    --url=${URL}
```
- NOTE: Typing a password right into bash is not safe, instead of env variables or password variables

Now run the command. Note that it 'fails' even when ran properly
- Can see if it ran succesfully with running `$?` immediately and checking if 1 or 0

## Dockerizing

Open the [Dockerfile](../2_docker_sql/Dockerfile) again.
- Need to add 'sqlalchemy' and 'psycopg2' in pip install line
    - `RUN pip install pandas sqlalchemy psycopg2`
- Need to install 'wget' to download CSV
    - `RUN apt-get install wget`
- Build it with `docker build -t taxi_ingest:v001`

Run docker image with:
```
docker run -it taxi_ingest:v001 \
    --user=root \
    --password=root \
    --host=pg-database \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_data \
    --url=${URL}
```
- Recall: We will need to run this within the Docker Network we made
- To close the docker image early, run `docker ps` in another window
    - Run `docker stop <Container ID>` or `docker stop <name>`

Also, to download the file faster, you can serve the current directory as a web GUI with Python.
- Run `python -m http.server` in the directory you want to serve
- Then you can get the file under '<local ip>:8000/' or whatever you specify

```
URL="http://172.25.8.156:8000/yellow_tripdata_2021-01.csv"

docker run -it \
    --network=pg-network \
    taxi_ingest:v001 \
    --user=root \
    --password=root \
    --host=pg-database \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_data \
    --url=${URL}
```
- While running this Docker line, when you check the lines in the DB, you should see 100,000 per update instead.

We have verified that this data pipeline works when Dockerized.
- Normally, you would have something like a Kubernetes job to run the Docker container
- The DB would have a different URL to your server or cloud