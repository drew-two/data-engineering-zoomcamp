# 1.3.2 - Creating GCP Infrastructure with Terraform

Recall: We were trying to create resources with Terraform
- The following modules: 
    - Google Cloud Storage (GCS): Data Lake
        - Like an S3 bucket (flat file system)
        - Data Lake is a way to store raw data in organized fashion
            - More directories and compressed with other file types (e.g. parquet)
    - BigQuery: Data Warehouse
        - Data is modelled into more structure with tables
- The following APIs:
    - General [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com)
    - The [IAM Service Account Credentials API](https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com)
- Execute again:
    - `gcloud auth application-default login`

## Files
- [`.terraform-version`](../1_terraform_gcp/terraform/.terraform-version) is the version of Terraform and what it needs
    - Can also use [tfenv](https://github.com/tfutils/tfenv), an environment manager
- [`main.tf`](../1_terraform_gcp/terraform/main.tf), the top level file of resources
    - Starts with `terraform` entry
        - `required_version` - terraform version needed
        - `backend` - where tf state file is (local, gcs, s3 etc), local for on-prem, or on your cloud
        - `required_providers` - optional if you define a provider already
            - Where terraform registry is picking publicly available providers
            - Let's you define configuration based on predefined configurations
            - Premade configurations for clouds (aws, gcp) or you can make your own (such as for multi-cloud or on-prem)
                - Where you get `google_storage_bucket` or `s3_bucket`
            - Think of it like loading a Python library
    - `provider` entry - loads predefined resource types and definitions
    - `resource` - abitrary infrastructure resource you defined or defined in your provider
        - Can be local resources or generally any cloud resource in a provider
        - Can see in [docs how these are defined](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket)
            - Gives examples and exact configuration if using GUI
            - Also links to official documentation
        - Variables defined from `variables.tf`
        - NOTE: not necessarily cloud agnostic
- [`variables.tf`](../1_terraform_gcp/terraform/variables.tf), parameters for the variable arguments in `main.tf`
    - More like constants
    - Passed at runtime
    - Ones with defaults are optional at runtime (fill in themselves)
        - Ones without default are required to be filled in at runtime
    - Examples:
        - `region` is based on the cloud region
            - Generally should be located nearby or in your area of business
            - Resources for a project should generally be in one location
                - May cost money for communication between regions
            - Can see regions for GCP [here](https://cloud.google.com/about/locations)
        - `bucket_name` - required, name for bucket
        - `storage_class` - optional, bucket storage class. Can default to "STANDARD"
        - `BQ_DATASET` - required, BigQuery dataset (like schema) raw data will be written to
        - `TABLE_NAME` - required, BIGQuery Table name


## Execution
1. `terraform init`: Initialize and install
    - Pulls in all configuration needed (from provider via web or otherwise)
    - Adds to state file
2. `terraform plan`: Match changes against previous state
    - Creates plan to actually make infrastructure that matches configuration
    - Compares to previous state if needed
        - E.g. you add resources, needs to detect if resource is new
    - Like a **dry run**
3. `terraform apply`: Apply changes to cloud
    - Any changes in the new plan will be detected and applied
4. `terraform destroy`: Remove your stack from cloud
    - Stops from adding to cost
    - **IMPORTANT** to remove at end of session or day

### Steps
1. Go to the folder where [main.tf](../1_terraform_gcp/terraform/) is and run `terraform init`
    - Pulls in hashicorp/google provider
    - Run ls -la and you will see new files
        - E.g. `.terraform` directory
            - Contains `terraform.tfstate` file
            - `.terraform.lock.hcl`
2. Run `terraform plan`
    - Only parameter mandated is `project` so we only write to our own projects
    - Plan shows 2 new resources
        - Google BigQuery
        - Datalake
    - Asks if you agree with creating these (dry run) but exits
3. Run `terraform apply`
    - Shows you the same as `terraform plan` but then asks if you want to perform the actions
    - Works quickly
    - Go back to [dashboard](https://console.cloud.google.com/welcome) for project
        - Go to Cloud Storage > Buckets
            - Name tried to keep name as possible (<bucket-name>_<project-name>)
            - Can see this in [main.tf](../1_terraform_gcp/terraform/): 
                ```
                resource "google_storage_bucket" "data-lake-bucket" {
                    name          = "${local.data_lake_bucket}_${var.project}" # Concatenating DL bucket & Project name for unique naming
                ````
        - Same thing for BiqQuery, BigQuery > Explorer
            - Can see resource `trips_data_all`


## Recap
Created GCP GCS bucket and BiqQuery resource
- Used terraform with minimum needed resources
- Ran very quickly

NOTE: Do NOT commit or push any credential files to GitHub
- Can add to .gitignore if needed