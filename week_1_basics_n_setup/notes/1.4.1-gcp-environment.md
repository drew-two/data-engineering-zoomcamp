# 1.4.1 - Setting up the Environment on Google Cloud (Cloud VM + SSH access)

## Set up Cloud
Go the the [dashboard](https://console.cloud.google.com/home/dashboard).
- Left side bar > Compute Engine > VM Instances
    - Enable it if need be
- Generate [ssh key in CLI](https://cloud.google.com/compute/docs/connect/create-ssh-keys)
    - Go to `~/.ssh` with `cd ~/.ssh/`
    - Run `ssh-keygen -t rsa -f ~/.ssh/KEY_FILENAME -C USERNAME -b 2048`
        - Replace KEY_FILENAME and USERNAME
    - Will create two keys, private key (no extension) and public key (.pub)
- Need to upload public key to Google cloud
    - Left hand bar > Settings > Metadata > SSH Keys tab
    - Hit 'Add SSH Key' and copy the one you just made in
        - ONLY the public key
    - Hit 'Save
- Every instances in this project (dte-de) will inhert the SSH keys

## Making VM
In the left hand bar go to Virtual Machines > VM instances
- Hit "CREATE INSTANCE"
    - Call it 'de-zoomcamp'
    - Choose region (I chose northamerica-northeast2 Toronto)
    - Scroll down and choose instance, maybe e2-standard-4
        - Monthly estimate
            $108.81
            That's about $0.15 hourly
        - Make sure to shut it off when not using it
    - Boot disk > Change > Public Images
        - Debian to Ubuntu 20.04 LTS
            - Just preference here
        - Update size to ~30 GB
    - NOTE: At the bottom "EQUIVALENT CODE" shows the GCP CLI code to make this
    - Hit 'CREATE'
        - MAKE SURE the free tier credit will be used (text is right above)
- We need to wait for the External IP

## Accessing VM
SSH to the new server
- Run `ssh -i ~/.ssh/gcp <USERNAME>@<External IP>`
    - Say 'yes'
- Can verify VM specs with `htop` command
- GCP CLI slready installed
    - Type `gcloud --version`
    - Fully featured

Install Anaconda now from [official page](https://www.anaconda.com/products/distribution)
    - Right-click Linux x86 link
    - Run `wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh | bash`
    - Read terms of service
        - Say yes
    - Confirm install location

Add config file to `.ssh/` with `touch .ssh/config`
- Can open it in VS Code with `code .`
- Add entry like the following:
    ```
    Host de-zoomcamp
        Hostname <External IP>
        User <Username>
        IdentityFile /home/<Username>/.ssh/gcp
        StrictHostKeyChecking no
     ```

Back in Anaconda: Say 'yes' to initializer.
- Look at .bashrc: `less ~/.bash.rc`
- Log out and log back in
    - Or run `source .bashrc`
- The `(base)` on the prompt means Anaconda is working
    - Can double check in Python shell

Install docker with `sudo apt install docker.io`
- Error: `E: Package 'docker.io' has no installation candidate`
- Need to run `sudo apt update` first to update package list
- Try to install Docker again

## Configuring VM for work
We want to configure VS Code so that we can code on the VM with a local copy of VS Code
- In the left side bar of VS Code go to extensions
- Find 'Remote - SSH' and install
- Now hit Ctrl+Shift+P to open command pallete and type
    - 'Connect to Host' and use the Remote - SSH command
    - Choose de-zoomcamp or whatever you called the GCP VM
- Clone [course repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/) into the VM

Try running docker in the VM with `docker run hello-world`
- Should fail because you are not superuser, but we do not want to have to run sudo every time
- Check the following link: https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md
    - Make docker group `sudo groupadd docker`
    - Add current user to docker group `sudo gpasswd -a $USER docker`
    - Log out and back in
    - Restart docker `sudo service docker restart`
- Running the hello world container should work now

Install Docker Compose from the [Github](https://github.com/docker/compose/releases)
- Get the Linux x86_64 link
- Make a folder to put executables `mkdir bin`
    - Move to the folder `cd bin`
    - Download Docker compose with wget
- Add executable permission with `chmod +x docker-compose-linux-x86_64`
    - Change name to just docker compose `mv docker-compose-linux-x86_64 docker-compose`
- Add directory to path:
    - Edit .bashrc `vi ~/.bashrc`
    - Add `export PATH="${HOME}/bin:${PATH}"`
    - Save and close; escape then `:wq` then enter
- Docker-compose should be accessible now

Bring up the docker-compose Postgres environment
- Go dir `cd data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql`
- Run `docker-compose up`
- Install pgcli with `pip install pgcli`
    - If this does not work you can install it from Anaconda
    - See [README.md](../2_docker_sql/README.md#cli-for-postgres)
- Log into DB with `pgcli -h localhost -U root -d ny_taxi`
    - If this throws an error, install `pip install mycli`

## Forwarding Ports
Go to the remote SSH VS Code window
- Hit ctrl+~, go to the PORTS tab
- Forward port 5432 to localhost:5432
    - Can forward 8080 now too for pgadmin
- Should be able to open pgAdmin from `http://localhost:5432` and access the DB

From the conda environment on the GCP VM, run Jupyter with `jupyter notebook`
- Forward this port if not automatically forwarded. 
    - You should be able to access this on `http://localhost:8888`
    - You need the link/token in the Jupyter startup text
- Open `week_1_basics_n_setup/2_docker_sql/upload-data.ipynb`

## Configuration

Got Remote SSH working with the following in User settings JSON:
```
"remote.SSH.path": "C:/Users/andre/ssh.bat",
```
- This file contains:
    ```
    C:\Windows\system32\wsl.exe ssh %*
    ```
- Which uses the WSL SSH directly as well as the default user's .ssh configuration

## Back to Jupyter
Download dataset to `week_1_basics_n_setup/2_docker_sql/`:
- ```bash
    wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz 
    ```
- Run the notebook. May need to add `.gz` to filenames
- Confirm the schema is in the database with pgcli and insert the first 100 rows

## Terraform
Install [Terraform](https://developer.hashicorp.com/terraform/downloads) to the VM:
- Use the Linux > Ubuntu tabs
    - Install
- Go to `week_1_basics_n_setup/1_terraform_gcp/terraform`
    - We want to apply the terraform but we do not have the GCP credentials JSON
    - For this we will use the sftp command
    - Go to the directory where your .json credentials file is
    - Run `sftp de-zoomcamp`
        - In the prompt, `mkdir .gc`, `cd .gc`
        - Run `put <json file>`
        - Exit with ctrl+D
- Go back to the terminal on the VM
- Configure GCP CLI:
    - We cannot use Oauth this time, so we need to use the JSON method
    - Run `export GOOGLE_APPLICATION_CREDENTIALS="${HOME}/.gc/<.json>"`
    - Run `gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS`
- Run `terraform init`, `terraform plan`
    - Enter project id (can also put in variables.tf)
    - Run `terraform apply`
        - Gcloud will complain because the state bucket was already created but the state file is on the location machine
- This is fine though, we get the idea

Can show down computer from console with `sudo shutdown now`.
- Can shutdown from the VM instance page: Options > STOP
- Won't be charged for running instance but will be charged (pennies) for the storage it is using

You can restart it from Options > START
- The external IP will change. You will need to update `~/.ssh/config`.
    - Say yes to fingerprint prompt
- Will have to bring up docker-compose and Jupyter notebook again

Can delete VM from Options > DELETE.
