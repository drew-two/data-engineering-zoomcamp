# 5.3.4 - SQL with Spark

Will take SQL from week 4 and apply with Spark.
- Make sure you have the data downloaded for 2020/2021 yellow/green tripdata
    - Can download with [download_data.sh](../code/download_data.sh) and [05_taxi_schema.ipynb](../code/05_taxi_schema.ipynb)

Make a new notebook `06_spark_sql.ipynb`
- Read all of green data with `spark.read.parquet('data/pq/green/*/*')`
    - Also load all of yellow
- Notice how the green and yellow datasets have slightly different schema
    - Will only use the overlapping columns
- Rename all the datetime columns so they are the same for both dataframes, then get the common columns
    - This is NOT an in-place operation
- First we verify that `df_green.select(common_columns)` does what we think it does
    - Then we apply F.lit to make the 'service_type' field (green or yellow) a literal (constant)
    - Select the yellow columns and set the literal as well
- Use `df.unionAll()` to union the newly made selected sets and check the count of 'service_type' with `.count().show()`

Use Spark SQL:
- Register the df as a temporary table to be able to use `spark.sql()` and have it apply to the table
    - `df_trips_data.registerTempTable('trip_data')`
- Apply SQL with:
    ```
    spark.sql("""
    SELECT * FROM trip_data LIMIT 10;
    """).show()
    ```
    - See we are calling the table with what name we registered the df with

Reformat the 'monthly_zone_revenue' model as SQL to use with Spark.
- ```
    SELECT 
        -- Reveneue grouping 
        PULocationID AS revenue_zone,
        date_trunc('month', pickup_datetime) AS revenue_month, 
        service_type, 

        -- Revenue calculation 
        SUM(fare_amount) AS revenue_monthly_fare,
        SUM(extra) AS revenue_monthly_extra,
        SUM(mta_tax) AS revenue_monthly_mta_tax,
        SUM(tip_amount) AS revenue_monthly_tip_amount,
        SUM(tolls_amount) AS revenue_monthly_tolls_amount,
        SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,
        SUM(total_amount) AS revenue_monthly_total_amount,
        SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,

        -- Additional calculations
        AVG(passenger_count) AS avg_montly_passenger_count,
        AVG(trip_distance) AS avg_montly_trip_distance
    FROM
        trips_data
    GROUP BY
        1, 2, 3
    ```
    - Run `df_result.show()` to force execution
- Save as parquet with `df_result.write.parquet('data/report/revenue/')` then see the Master UI
    - 14 tasks with on ~460MB
    - Can see the DAG shows the parquet scanning happens in parallel (yellow and green)
        - Then Unions, applies transformation and cleanup, then writes to parquet
- See the directory saved to. Ends up as 342 KB file.
    -  Only one partition, while Alexey gets 200. Dependent on Spark version/parameters/etc
- Save with `df_result.coalesce(1).write.parquet('../notebooks/data/test/revenue/', mode='overwrite')` instead.

So we wrote the results locally, but imitating a data lake.
- Earlier Alexey said to use SQL if you can, but here we ran SQL using Spark
- If we can do things easier may as well use Spark if you can