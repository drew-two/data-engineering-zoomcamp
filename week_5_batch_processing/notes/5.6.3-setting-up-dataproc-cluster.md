# 5.6.3 - Setting up a Dataproc Cluster

## Overview
- Creating a Cluster
- Running a Spark job with Dataproc (Data Processing - GCP Product)
- Submitting the job with the cloud SDK

## Creating a Cluster
Go to UI: https://console.cloud.google.com/dataproc/clusters.
- Enable if not done before

Click on Create cluster
- Make it in compute engine
- Call it something like `de-zoomcamp-cluster`
- Keep it in the same region as your bucket
- Cluster type: 'Standard'
- Optional components:
    - Jupyter Notebook
    - Docker
- Click Configure Nodes
    - Set Series 'N1'
    - Set Machine type 'n1-standard-4'
- Click CREATE.

Also creates a VM that the cluster is using.
- Would be called 'de-zoomcamp-cluster-m'
- Could connect to it, but don't need to

Recall how we connected to GCS with the Hadoop connector
- DataProc is already configured to connect to GCS so we do not have to consider this

## Running a Spark Job
Go to the cluster, hit Submit Job
- Job type: PySpark
- To specify Python file, need to upload somewhere like GCS bucket
    - Upload `06_spark_sql.py` to bucket
    - I.e. `gsutil cp 06_spark_sql.py  gs://<bucket-name>/code/06_spark_sql.py`
    - Specify this gs path for the Python file
- Arguments: recall the commands we used when we ran `spark-submit`
    - Need to change to specify data from buckets rather than locally
        ```
        --input_green=gs://<bucket-name>/pq/green/2021/*/ \
        --input_yellow=gs://<bucket-name>/pq/yellow/2021/*/ \
        --output=gs://<bucket-name>/report-2021
        ```
    - Need to hit enter between each argument
    - Lots of logging, says it prepares directory to write to
- Check results in the bucket, see folder `code/`
    - Should see `report-2021/` with the parquet output

## Submitting the job with Cloud SDK
Submitting via web UI is not convenient, need a different way from CLI or Airflow.
- Requires Google SDK
- 3 ways to submit job:
    1. Web UI
    2. Google Cloud SDK
    3. REST API
        - Click on the Job details > CONFIGURATION > EQUIVALENT REST

Submitting via Google Cloud SDK: https://cloud.google.com/dataproc/docs/guides/submit-job#dataproc-submit-job-gcloud
- Command:
    - ```
        gcloud dataproc jobs submit pyspark \
            --cluster=de-zoomcamp-cluster \
            --region=northamerica-northeast1 \
            gs://<bucket-name>/code/06_spark_sql.py \
                -- \
                    --input_green=gs://<bucket-name>/pq/green/2021/*/ \
                    --input_yellow=gs://<bucket-name>/pq/yellow/2021/*/ \
                    --output=gs://<bucket-name>/report-2021
        ```
    - Run in the terminal
- This fails, because our service account does not have permissions to submit to DataProc

Go to IAM and Admin: https://console.cloud.google.com/iam-admin/iam
- NOTE: Should be using different service accounts, e.g. one for master, one for executors
- Will be using the same one for simplicity
- Add 'Dataproc Administrator'

Try running the command again. Should work this time
- Job is submitted, just waiting for outputs now.
- Same as using web UI, just from the CLI.

Run this again but with the arguments for the 2020 data.
- ```
    gcloud dataproc jobs submit pyspark \
        --cluster=de-zoomcamp-cluster \
        --region=northamerica-northeast1 \
        gs://<bucket-name>/code/06_spark_sql.py \
            -- \
                --input_green=gs://<bucket-name>/pq/green/2020/*/ \
                --input_yellow=gs://<bucket-name>/pq/yellow/2020/*/ \
                --output=gs://<bucket-name>/report-2020
    ```
- Should work, and have see `report-2020` in the bucket now.

For our projects, we would be more interested in putting this information into a data lake
- Now that this is in GCS, we can read this to a normal BigQuery table
- HOWEVER, we can have Spark write directly to a BigQuery table.