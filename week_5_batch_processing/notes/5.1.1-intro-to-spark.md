# 5.1.2 - Introduction to Spark

Apache Spark: large-scale, multi-language engine for executing data pipeline on one or many machines
- Spark usually takes some data from a database, operates on data, then writes it to a DB
- Spark is an engine so it manages a cluster where it runs the jobs. Can have 10s or 1000s of machines to operate on data
- Can use Python, Java and Scala - written in Scala, so "native" language is Scala
    - PySpark is pretty popular, and usually the preferred way
    - Otherwise Scala if more flexibility is needed
- Used to execute batch jobs but can also be used for streaming (won't do streaming this week)
    - Can see streaming data as a sequence of small batches and apply techniques for processing this stream as you would do with batches

## When to Use Spark?
Typically used when data in a data lake.
- Usually some location in S3/GCS with a bunch of CSV/Parquet
- Spark takes these files, does something, and puts the data back into a DL

When files are in a format like CSV or Parquet, it is not convenient to use SQL, so we use Spark.
- These days you can run SQL on data lake directly with tools like:
    - Hive 
    - Presto (AWS Athena)
- Then write the data back to a data lake
- If you can express your job as a SQL query, you should use Hive/Presto/BigQuery (for CSV/Parquet)

Say you need more flexibility, or code becomes too difficult to module, want to have unit tests, etc. Things not easily/expressable with SQL
- Then you would want to use Spark
- These are typically **machine learning** jobs

Typical Workflow:
- Some raw data, gets written to data lake, apply transformations with SQL (Athena), apply more complicated operations with Spark, then use Python for training ML model
- Can have another flow for actually using ML model
    - E.g. Take the Spark output, and in another Spark instance take the generated model and apply it
        - Result goes to a data lake.
- Most preprocessing happening in SQL
