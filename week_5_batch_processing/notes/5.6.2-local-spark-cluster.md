# 5.6.2 - Creating a Local Spark Cluster

## Overview
- Creating the cluster
- Turning the notebook into a script
- Using spark-submit for submitting Spark jobs

## Creating The Cluster
Done in the VM but we are considering this local as it is very close.
- Using [06_spark_sql.ipynb](../code/06_spark_sql.ipynb) for this example
- Recall that when we create a SparkSession, we create a local Spark cluster and connect it
    - This is the Master cluster we've been using
- Shut down the kernel. This ends the cluster instance.

Creating a Spark cluster outside of the notebook. Using: https://spark.apache.org/docs/latest/spark-standalone.html
- Run `~/spark/spark-3.3.2-bin-hadoop3/sbin/start-masters.sh`
- Opens on port 8080 this time.

Make new file [10_spark_local.ipynb](../notebook/10_spark_local.ipynb)
- Copy code from [09_spark_gcs.ipynb](../code/09_spark_gcs.ipynb)
- When calling it from Python, instead of "local[*]", use the URL from the cluste webpage
- When you try to use Spark, it will tell you there are no workers or they have insufficient resources.
    - Need to register some workers. Can run `~/spark/spark-3.3.2-bin-hadoop3/sbin/start-worker.sh <master-spark-URL>`
- Spark should know be able to run as it has workers

Add the common columns from [06_spark_sql.py](../code/06_spark_sql.py) and remove original call

## Turning Notebook into Script
Run `06_spark_sql.py`
- Open this new file now. Clean up formatting
- Run in terminal

Notice that as we did not specify resources for the notebook, it used all resources.
- Need to add more workers or close notebook. We close notebook.

There are some things we can improve. What if we want to only run one taxi type, or a certain year etc.
- Adjust to use command line interface. Will use arg parse package.
- Import argparse and clean up imports.
    - Add arguments for `--input--green`, `--input-yellow`, `--output`. Make them all required
    - Assign to variables
- Run command: 
    ```
    python 06_spark_sql.py \
        --input_green=data/pq/green/2020/*/ \
        --input_yellow=data/pq/yellow/2020/*/ \
        --output=data/report-2020
    ```
- Check folder to see the output `data/report/revenue/report-2020`

## Spark Submit
Imagine we have multiple clusters. Works for one cluster but if we have others or use Airflow, hardcoding is not practical.
- Could also want to specify number of executors and amount of RAM

Remove Spark builder master line, keep the rest.
- Usage:
    ```
    ./bin/spark-submit \
        --class <main-class> \
        --master <master-url> \
        --deploy-mode <deploy-mode> \
        --conf <key>=<value> \
        ... # other options
        <application-jar> \
        [application-arguments]
    ```
    - Can replace <application-jar> with a *.py file
- We would run:
    ```
    URL="spark://de-zoomcamp.northamerica-northeast2-a.c.dtc-de-377105.internal:7077" 

    spark-submit \
        --master="${URL}" \
        06_spark_sql.py \
            --input_green=data/pq/green/2020/*/ \
            --input_yellow=data/pq/yellow/2020/*/ \
            --output=data/report-2020
    ```
    - See a lot more information, like about Spark allocation and lots of logging.
- Do not hardcode Spark URL, worker spec, etc in code in practice.
- This is how you would configure to run for different months or days with Airflow.

Stopping workers and master:
- Master: `./sbin/stop-master.sh`
- Worker: `./sbin/stop-worker.sh`