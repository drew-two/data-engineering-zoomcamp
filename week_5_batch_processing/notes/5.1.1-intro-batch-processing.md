# 5.1.1 - Introduction to Batch processing

## Week Overview
Will spend most of week talking about Spark. Will use Linux VM on GCP and use Spark there.
- Use PySpark and look at different features of Spark
- Then RDDs (relatively old concept), DF vs RDD etc
- Internals of Spark
- Spark and Docker, Spark in the Cloud
- Connecting Spark to a DWH

## Batch Vs Streaming
Two main ways to process data: Batch vs Streaming
- Will focus on Streaming in Week 6

Batch (Existing data), ~90% jobs are batch
- Say we have a DB, e.g. Taxi Trips DB
- We take all the data for Jan 15 in the DB
- A job takes all this data, and produces the entire dataset

Streaming (Real-time), ~10% are streaming
- Say we are in NY, we hail a taxi
- The computer onboard sends a signal RIDE_STARTS saying the ride has started
- This event is sent to a data stream, and something processes this data, putting it into another datastream
- This all happens on the fly

## Batch Jobs
Typically done over a time frame. Can be pretty granular:
- Weekly
- Daily
- Hourly
- 3 timer per hour
- Every 5 minutes

Usually daily or hourly though. 

Technologies normally used:
- Python scripts (e.g. Week 1 ingestion script w/ monthly intervals)
    - Can be ran anywhere. E.g Kubernetes, AWS Batch
- SQL (Week 4)
- Spark
- Flink

Example Workflow:
- Have a data lake, Python job puts it in a DWH, SQL (e.g. dbt) performs some data preparation, Run Spark, Run Python again

## Advantages of Batch Jobs
- Convenient, easy to manage.
- Retry steps of workflow
    - E.g. Timeout eventually and restart job
- Scaling
    - Can scale hardware of each job separately

- Disadvantage: delay
    - Know we have to wait on data
    - Have to wait on each step of the job, overhead in workflow etc
    - E.g We need data from previous hour. Cannot use until hour ends, and then after waiting 20 minutes waiting for pipeline
        - Thus we need to wait 90 mins before using data
    - Can be solved with streaming, but usually not important. Can often wait a few hours or day before using data
