# 5.2.1 - (Optional) Installing Spark on Linux

Start your GCP VM: https://console.cloud.google.com/compute/instances
- OR use WSL

See [linux.md](../setup/linux.md)
- Can do with WSL or GCP VM

We intentionally use spark 3.0.3 on Windows for compatibility.

We run a test command in the spark shell (scala code) just to make sure Spark is working.

Add the following commands to your `~/.bashrc` file.

Commands:
```
export JAVA_HOME="${HOME}/spark/jdk-11.0.2"
export PATH="${JAVA_HOME}/bin:${PATH}"

export SPARK_HOME="${HOME}/spark/spark-3.3.2-bin-hadoop3"
export PATH="${SPARK_HOME}/bin:${PATH}"
```

## Trying PySpark

Go to [pyspark.md](../setup/pyspark.md)
- PySpark needs some exports

Create directory `/notebooks` and open Jupyter. Open a remote host in VS Code.
- Open port 8888

Use [pyspark.md](../setup/pyspark.md) and follow along in the notebook.

Also forward port 4040. This is the Spark master controller GUI
- This was the Spark session we made in the notebook