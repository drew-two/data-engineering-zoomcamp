# 5.3.1 - First Look at Spark/PySpark

## Overview
- Reading CSV files
- Partitions
- Saving data to Parquet for local experiments
- Spark Master UI

Alexey moved `notebooks` folder to be `week_5_batch_processing/code`
- I will just move the whole notebook so it is `week_5_batch_processing/notebooks`

Make new file, `04_pyspark.ipynb`.
- Copy in the imports.
- Open a Spark master session
- Download the For Hire High Volume NYC TLC data for Jan 2021
    - `wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz`
- Open the file with Spark
- Going to http://localhost:4040 we can see the Spark jobs

- Looking at the file with ead, we can see everything is a string
    - Confirm with `df.schema`
- Recall in week 1, we used Pandas to infer a schema then used that schema in our local DB
- Unzip the file with `!gzip -kd fhvhv_tripdata_2021-01.csv.gz`
- Run `!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv`
    - Keeping the first 1001 rows
    - Check this with `!head head.csv`
        - First 10 rows
    - Confirm with `wc -l head.csv`
- Import the DataFrame with `df_pandas = pd.read_csv('head.csv')`
    - Can read this into Spark with `spark.createDataFrame(df_pandas)`
- Show the schema. We want to adjust this and load it as the files schema
    - This is Scala code. Want to convert to Python
        ```
        types.StructType([
            types.StructField('hvfhs_license_num', types.StringType(), True), 
            types.StructField('dispatching_base_num', types.StringType(), True), 
            types.StructField('pickup_datetime', types.TimestampType(), True), 
            types.StructField('dropoff_datetime', types.TimestampType(), True), 
            types.StructField('PULocationID', types.IntegerType(), True), 
            types.StructField('DOLocationID', types.IntegerType(), True), 
            types.StructField('SR_Flag', types.DoubleType(), True)
        ])
        ```
    - Note that SR_Flag is mostly empty, we will convert to StringType()
        - Also set Flag to nullable
- Load with it with `spark.read()` using option `.schema()` with the schema variable

Now want to save as Parquet. However, this is a large file and one large Parquet file is not good.
- The inside of a Spark cluster can be assumed to multiple computers doing computational wark
    - These pull files from some Data Lake
    - Say we are using GCS, `gc://bucket-name/folder/../` with a bunch of files
    - Each executor on Spark will get one of the files
    - Note that this does not assign every file.
        - The leftover files will be picked up by whatever executors gets leftover first
- But image if there was just one large file in the data lake
    - Then the rest of the Spark executors will be idle as the data can only be read sequentially.
- Not good, so we want a bunch of smaller files to be able to execute parallelly

So we want to break up our CSV file somehow.
- In Spark, these multiples files have a name. Called **partitions**
- Say we take one large file and split it into 24 partitions?
- We can run `df.repartition()` to split a file. Do so with 24 partitions and ovewrite DataFrame.
    - So we can take a DataFrame with many partitions, which would be the multiple files in GCS
    - Spark will make a DataFrame internally with as many files are in the GCS folder
- Note that this command is a **lazy command**. It will be partitioned when we do something with it.
    - Can confirm this in the Spark Master UI

Write the parquet out:
- `df.write.parquet('fhvhv/2021/01')`
    - Looks like there are already 6 partitions in the file. And 4 cores
    - Can see the jobs in the Spark Master UI
    - Can see the DAG for the job and the subtasks
- Takes 16s, 1 stage and 24 tasks
- Take a look at the folder we have made
    - If still during the job, we can s
    - Can see the SUCCESS confirmation and the 24 parts
        - Can't confirm the job is finished without this. Like a commit message
    - We see that Spark is using the 'snappy' compression algorithm
- If we try running this again, Spark will complain saying the path already exists
    - Can add `mode='overwrite'` into `df.write.parquet()` if we want.
