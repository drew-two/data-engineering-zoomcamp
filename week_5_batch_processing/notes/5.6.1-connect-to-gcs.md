# 5.6.1 - Connecting to Google Cloud Storage

## Overview
- Uploading data to GCS
- Connecting Spark jobs to GCS

## Uploading Data to GCS
Uploading the parquet files we have generated earlier this week to GCS.
- `gsutil -m cp -r pq/ gs://<bucket name>/pq`

Downloading GCS Connector for Hadoop:
- https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#getting_the_connector
- Gives location: `gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar`
    - Not version for Spark but for this GCS connector
- Make directory `lib` and save connector to folder

Make notebook [`09_spark_gcs.ipynb`](../notebook/09_spark_gcs.ipynb)
- Connecting to GCS: Copy from [09_spark_gcs.ipynb](../code/09_spark_gcs.ipynb)
    - Copy the credentials.json into the remote VM
- Use the code to start Spark with the Hadoop environmental variables
    - Specify path to credentials as `credentials_location`
- Now copy over Spark setting more Hadoop variables
    - Specifies what implementation for gsutil link, coming from the .jar file
- Now need to start Spark Session using the conf we were using for the Spark Context
- Can show or count DataFrame to confirm.