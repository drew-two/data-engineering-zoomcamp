# 5.4.1 - Anatomy of a Spark Cluster

## Spark Cluster
Have been experimenting with everything locally
- All executors on one computer.
- When setting up Spark conect, we specify the master. This happens in:
    ```
    spark = SparkSession.builder \
    .master("local[*]") \
    .appName('test') \
    .getOrCreate()
    ```

Normal workflow:
- Create a script in Java/Python/Scala on laptop
- Have external cluster with a master executor
- Submit the script to Spark master via URL (usually http://localhost:4040)
    - Specify some parameters like resources we need, restraints, etc
- Executors on Spark cluster actually do the work, coordinated by Spark master
    - E.g. Send job to executors 1-5 of 6
- Master executor always running. If an executor goes down, Master reorganizes tasks to other executors.
- Executors first pull the data before they can process the data
    - E.g. Dataframe with *k* partitions (each is a parquet file).
        - When we submit a job to Spark master, each exectuor pulls a partition.
        - They work through the partition, mark the task as completed, then get another task with another partition
    - Dataframes usually lives in S3, GCP, etc
    - Back in the day, HDFS and Hadoop were pretty popular
        - Had a data lake with files actually stored on executors
        - Redundant, same partitions can be stored on multiple computers
            - But, if data is lost it is still redundant elsewhere
        - Idea was to have data locality, downloading code and data at the same time
        - This was because data could be quite large, e.g. 10MB, with small code, like 1 MB.
    - These days, because we have cheap cloud storage, that usually lives in the same datacenter, thus downloading 100 MB is very fast.
        - A little slower than reading from local like Hadoop/HDFS, but has advantages of having data external and no overhead

## Summary
Driver (user, orchestrator) submits job to Spark cluster
- Spark master executor keeps track of jobs and machines, and splits jobs to assign pieces to executors in parallel
- Data lakes are usually saved external now as cloud storage is cheap and fast now.