# 2.2.4 - From Google Cloud Storage to Big Query

Recap: Data from web and putting in GCS Bucket - treating as Data Lake
- Now want to get it into a Data Warehouse

## BigQuery Upload Script

Make file [etl_gcs_to_bq.py](../flows/02_gcp/etl_gcs_to_bq.py)
- Get the imports from [etl_web_to_gcs.py](../flows/01_start/etl_web_to_gcs.py)
- Add `if __name__ == '__main__'` clause
- Make `flow()` function `etl_gcs_to_bq()` and call new `@task` function `extract_from_gcs` with 3 retries
    - Task function takes color, year and month as input, returns Path
        - Use case: If other users want to call the data from GCS, need well defined path/URL
    - Task pulls data from GCS Bucket using the f-string from [etl_web_to_gcs.py](../flows/01_start/etl_web_to_gcs.py)
    - Save file to `week_2_workflow_orchestration/data/` directory
- Run script to check

Create `@task()` function `transform()` with 3 retries
- Add the passenger count clean up code from [ingest_data.py](../flows/01_start/ingest_data.py)
- Use `df.to_gbq()` functions with the parameters:
    - `destination_table`
    - `project_id`

## Make BigQuery Instance

Go to https://console.cloud.google.com/bigquery > '+ ADD DATA'
- Choose GCS as source, select from Bucket > Browse and find the file
- Choose the project
- Choose the Dataset, chose something like 'dezoomcamp'
- Name table, chose 'rides'
- Hit `Create Table`

Back in the BigQuery UI you will see `dezoomcamp/rides` as a data source
- Shows the schema of the data automatically
- Open a Query in a new tab
- Run `DELETE FROM `dtc-de-377105.dezoomcamp.rides` WHERE true;` to empty the data

## Back to Script
Finish `df.to_gbq()` where the parameters are:
- destination_table='dezoomcamp.rides'
- project_id='dtc-de-377105'
- chunksize=500_000,
- if_exists="append"

Go to the Prefect Blocks page http://localhost:4200/blocks
- Get the code to load the block for our GCP credentials
- Add it in the function before calling `df.to_gbq()`
    - Add the parameter `credentials=gcp_credentials_block.get_credentials_from_service_account()` 
- Run the script to test

Go back to BiqQuery UI and open new query tab and run the default to see data
- Count rows - should be 1,343,039