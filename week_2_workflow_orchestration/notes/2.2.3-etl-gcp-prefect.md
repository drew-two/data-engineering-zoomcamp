# 2.2.3 - ETL with GCP & Prefect

Use the conda env that uses [requirements.txt](../requirements.txt)
- `conda activate zoomcamp`
- Run `prefect orion start`
    - Go to dashboard at https://localhost:4200/ in the background

Make a new folder `week_2_workflow_orchestration/flows/`
- Make subfolders `01_start` and `02_gcp`
- Make a new file `01_start/etl_web_to_gcs.py`

We are going to have multiple flow functions that will call flow functions
- It will take the taxi data, do some cleanup, and save as **parquet** in our data lake in GCP

## ETL transfer Script
Go to `01_start/etl_web_to_gcs.py`
- Add a new flow, `etl_web_to_gcs() -> None:`
- Parameterize dataset file then dataset URL with variables for color, year and month
- Call function `fetch`
    - Give this the `@task` decorator with 3 retries
        - We will make this randomly fail sometimes to demonstrate
    - Retries also have waiting period, exponential backoff etc
    - Just call `pd.read_csv` to the URL

Cleaning up dataset - add function `clean()` with decorator `@task(log_prints=True)`
- `log_prints=True` so the prints actually print to stdout, do not normally in Prefect
- Add type hints (input and output both dataframes)
- Same cleanup to dataframe we have been doing before
- Print the first two rows, number of records, and dtypes of columns

Saving file as parque - add function `write_local()` with decorate `@task()`
- Takes in dataframe, taxi color and dataset name; returns Pathlib Path object
- Using Pathlib Path to write to `data/{color}/{dataset_name}`
- Save to parquet with `pd.to_parquet()` using gzip compression

## Writing to GCS Bucket
Create bucket if you haven't at https://console.cloud.google.com/storage/browser/

Open Prefect in the browser at http://localhost:4200 > Blocks
- Want to create Block for GCS ready for us before using script
- Run `prefect block register -m prefect_gcp` in CLI
    - We see have Register Blocks for GCP Credentials and GCS Bucket
- In the Blocks screen in the browser, hit the + sign
    - Add + "GCS Bucket"
    - Name it something like 'zoom-gcs'
    - Give it the name of the bucket we are using
    - Notice there is an option to set GCP Credentials, hit Add +
        - Now we are creating a new GCP Credentials block
            - Make sure the role has BigQuery and Storage permissions
            - Can just give them full admin for now
        - Name it something like 'zoom-gcp-creds'
        - Now we have to enter the service account file path or the file contents
        - Hit Create
    - Choose the GCP credentials from the UI
    - Hit Create
- Now we can copy the code snippet to use in our script
    - There is a typo here, Bucket is repeated on the first line

Put the copied code in the `write_gcs()` function of [etl_web_to_gcs.py](../flows/01_start/etl_web_to_gcs.py)
- Upload to GCS with the object and the path we passed into the function
- If it passes, check the flow log in Prefect, and the GCS Bucket that it uploaded.