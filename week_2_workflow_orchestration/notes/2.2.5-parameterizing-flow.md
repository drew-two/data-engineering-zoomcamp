# 2.2.5 - Parametrizing Flow & Deployments with ETL into GCS flow

Make new file [parameterized_flow.py](../flows/03_deployments/parameterized_flow.py)
- Copy over the code from [etl_web_to_gcs.py](../flows/02_gcp/etl_web_to_gcs.py)

Want the parameters of the data file to not be hardcoded so we can easily choose the file/trip type
- Edit `etl_web_to_gcs()` to be `etl_web_to_gcs(year: int, month: int, color: str)`

Let's create a parent flow that calls this entire process 3 times, for a different month each time
- Make `@flow()` function `etl_parent_flow(months: list[int] = [1, 2], year: int = 2021, color: str = "yellow")`
- Call it from the main clause with color = yellow, months = [1, 2, 3] and year = 2021

Now we run the script. Make sure prefect orion is running, and the GCS bucket is created and credentials configured
- Run `python parameterized_flow.py`
- Look in [Prefect UI](http://localhost:4200/flow-runs) to confirm the flow runs all worked

## Configure Prefect Deployment
Server-side concept that encapsulates a flow so it can be triggered/scheduled and run from the API
- Need two things:
    - Flow code
    - Deployment definition
        - Let's Prefect know about environment (e.g. Docker, Kubernetes, ECS)
        - Code may not be locally stored, Prefect may need to pull it from somewhere
        - Can also provide different parameters to flow code
- Two ways to make deployment definition:
    - Via CLI
    - Via Python
- When building deployment there are many options through CLI
    - E.g. name, tag, work-queue, cron schedule, interval, whether to apply immediately, etc

In the terminal, run with `prefect deployment build`
- Run `prefect deployment build ./parameterized_flow.py:etl_parent_flow -n "Parameterized ETL"`
    - Specifies the code, and the entry flow function (as there could be many) and names the Deployment
- This makes files [etl_parent_flow-deployment.yaml](../etl_parent_flow-deployment.yaml) and [.prefectignore](../.prefectignore)
    - Edit `parameters` to be `parameters: { "color": "yellow", "months": [1, 2, 3], "year": 2021}`
- Now run `prefect deployment apply etl_parent_flow-deployment.yaml`
    - Should run without error

Can see the Deployment now exists in the UI
- Can edit deployment (e.g add description)
    - Can choose Work Queue, tags, Scheduling and choose the parameters for the parent flow function
- Back out, and we can see that we can Run the flow. Trigger a 'Quick Run'
- Can see the scheduled Flow run in the [Flow runs page](http://localhost:4200/flows/)
    - Why is it scheduled? **There is no agent to pick up the run**
- **Work Queue**: pausable queue that schedules all the deployments to be picked up by agents
    - **Agent**: Very lightweight Python process living in your execution environment
    - Useful because you can set up agents in different environments
        - E.g. Docker, Kubernetes, Cloud instances etc.
- By clicking on the Work Queue in the UI we can get the code to start an agent in a terminal
    - Should start running right away, can see in the UI

## Notifications

In any case, now that we can trigger our flow runs via deployment, we should set up notifications
- Go to [Notifications](http://localhost:4200/notifications/) in the left-hand bar of the UI
    - Hit 'Add Notification +'
    - Specify Run state as 'Failed'
    - Note that you can set up tags, e.g. by project, business unit, etc
    - Can specify the webhook, such as Microsoft Teams or Twilio SMS
    - Put random characters in Webhook URL
    - Hit 'Create'