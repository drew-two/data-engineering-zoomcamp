# 2.2.2 - Introduction to Prefect Concepts

Take a look at [ingest_data.py](../../week_1_basics_n_setup/2_docker_sql/ingest_data.py)
- It **transforms** data and applies the SQL to a Postgres server
- Use parameters:
    - user = postgres
    - password = admin
    - host = localhost
    - port = 5432
    - db = ny_taxi
    - table_name = yellow_taxi_trips
    - url = https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz
- We will transform this into being orchestrated with Prefect

Create an anaconda environment for this
- E.g. `conda create -n zoomcamp python=3.9`
- Activate `conda activate zoomcamp`
- Install requirements with `pip install -r requirements.txt`
- Verify prefect with `prefect version`
- Run it now in a terminal with `prefect orion start`

Go back to the Python script (copied to [ingest_data.py](../ingest_data.py))
- Bring up Postgres and pgadmin: Run `docker-compose up` in `week_1_basics_n_setup/notes` 
- Populate data if need be:
    - Run with `python ingest_data.py` or 
        ```
        URL="http://172.25.8.156:8000/yellow_tripdata_2021-01.csv"

        docker run -it \
            --network=pg-network \
            taxi_ingest:v001 \
            --user=root \
            --password=root \
            --host=pg-database \
            --port=5432 \
            --db=ny_taxi \
            --table_name=yellow_taxi_data \
            --url=${URL}
        ```
- Open pgAdmin from `localhost:8080` and open the Query Tool for the DB
    - Run `SELECT * FROM yellow_taxi_trips;`

## Conversion to Prefect
Add imports `from prefect import flow, task`
- `flow` is the most basic Python object, container of workflow logic that allows interaction with workflow
    - Like functions, take input, do work, give outputs
    - Add `@flow(name="Ingest Flow")` decorator to a new main function
        - Move all of the `if __name__ == '__main__':` to this, and just call `main_flow()` there now
- Change the old main function to `ingest_data(params)` and add `@task()`
    - Tasks are not necessary but can receive metadata from upstream dependencies before run
        - E.g. Allows task to wait before execution of another task
    - Add `(log_prints=True, retries=3)` for automatic retries and logging
    - Remove the while loop
        - It will only add the first 100,000 rows now unless you change  the read_csv line to:
            `df_iter = pd.read_csv(csv_name, iterator=True)#, chunksize=100000)`

To make sure this is running as a flow, drop the table from pgAdmin
- Then run flow with `python ingest_data.py`

## ETL Conversion

### Extract Data

Start by breaking up `ingest_data()`
- By breaking this up we get more visibility into each of the tasks
- Add new task `def extract_data(url)` with `log_prints=True, retries=3`
    - Add the lines from the first comment to the datetime conversion rows
    - Move the create engine line back to `ingest_data()`
    - Return the dataframe
    - Call the function from `main_flow()`
    - One of the benefits of tasks, is that you can **cache results** to save on time on repeated actions
        - Add import `from prefect.tasks import task_input_hash`
        - Change task decorator to `@task(log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))`
        - Add import `from datetime import timedelta`

### Transform Data
- Add new function `transform_data(df)` with `log_prints=True`
    - Add `print(f"pre:missing passenger count: {df['passenger_count'].isin([0]).sum()})")`
        - Checks missing passenger counts (every taxi trip should have passenger count > 0)
    - Drop these rows with `df = df[df['passenger_count'] != 0]`
    - Confirm there is no empty passenger rides with `print(f"post:missing passenger count: {df['passenger_count'].isin([0]).sum()})")`
    - Return the dataframe
    - Call the function from `ingest_data()`

### Load Data
- Edit `ingest_data()` to take in a dataframe as a parameter and call from `main_flow()`

Recap: We imported prefect flow and task, and prefect.tasks.task_input_hash as a cache key, and datetime.timedelta
- Added two new tasks, `extract_data()` and `transform_data()`
- Changed `ingest_data()` to just load the dataframe into the database

Run it again `python ingest_data.py`

## Parameterization and Subflows
Adding more Prefect functions like parameterization to the flow, take different DB parameters etc
- Change `main_flow()` to `main_flow(table_name: str)` and add it to the `if __name__ == '__main__':` clause

Add subflow by making a new flow decorator `@flow(name="Subflow", log_prints=True)`
- Add function `def log_subflow(table_name: str):`
    - Add `print(f"Logging Subflow for: {table_name}")`
- Call this function from `main_flow()`

Run python script again
- Can see subflow in prefect run

## Prefect Orion UI
Open in browser at http://localhost:4200
- Can see a log of all the flows and tasks we were just running
- Can click a flow run and see the logs, task runs, subflows
    - Can see that the subflow run got the parameter

Can see everything in left side bar
- See Notifications: Can trigger based on basically any Task state
    - Can send these externally like MS Teams webhooks
- Task Run Concurrency lets you limit how much and what can run sequentially or concurrently
    - E.g. mapping over 100 inputs and only want to run 10 at a time
- See Blocks: interface for interacting with external data stores
    - E.g. Azure, AWS, BigQuery, local Docker etc
    - Block names are **immutable**, so you can use these block names in code without changing code
    - Clicking a block gives you the example code
        - Can actually build blocks upon other blocks

Open bash terminal.
- We installed "Prefect Collections" such as GCP cloud storage and sqlalchemy
- To use these, go to Blocks > Add Blocks > Search sqlalchemy
    - Choose SyncDriver 'postgresql+psycopg2'
    - Create connector and add the DB information for our Postgres DB
    - We do not need any other settings
    - Save and create block

Now we will edit [ingest_data.py](../ingest_data.py) to add the sqlalchemy connector
- Add the code from the driver block for importing
- Can open connection with `connection_block = SqlAlchemyConnector.load()`
    - Add in the string of whatever you named the block
- Replace all connection code with `with connection_block.get_connection(begin=False) as engine:`
    - Move dataframe code into this `with` block
- Now we can remove all the DB info (except table name) from `main_flow()`

Run the script again.